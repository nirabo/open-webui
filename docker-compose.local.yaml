name: ollama-webui
services:
  litellm:
    command:
      - --config
      - /conf/config.yaml
      - --port
      - "8000"
      - --num_workers
      - "8"
    image: litellm-proxy
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8000
        published: "12345"
        protocol: tcp
    volumes:
      - type: bind
        source: /home/lpetrov/projects/sandbox/ai/ollama-webui/my/litellm.proxy.yaml
        target: /conf/config.yaml
        read_only: true
        bind:
          create_host_path: true
  ollama:
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities:
                - gpu
              driver: nvidia
              count: 1
    image: ollama/ollama:latest
    networks:
      default: null
    ports:
      - mode: ingress
        target: 11434
        published: "11434"
        protocol: tcp
    pull_policy: always
    restart: unless-stopped
    tty: true
    volumes:
      - type: bind
        source: /media/lpetrov/4TB/opt/ollama-data
        target: /root/.ollama
        bind:
          create_host_path: true
  ollama-webui:
    build:
      context: /home/lpetrov/projects/sandbox/ai/ollama-webui
      dockerfile: Dockerfile
      args:
        OLLAMA_API_BASE_URL: /ollama/api
    container_name: ollama-webui
    depends_on:
      ollama:
        condition: service_started
        required: true
    environment:
      OLLAMA_API_BASE_URL: http://ollama:11434/api
    extra_hosts:
      - host.docker.internal=host-gateway
    image: ghcr.io/ollama-webui/ollama-webui:main
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8080
        published: "3000"
        protocol: tcp
    restart: unless-stopped
    volumes:
      - type: volume
        source: ollama-webui
        target: /app/backend/data
        volume: {}
networks:
  default:
    name: ollama-webui_default
volumes:
  ollama-webui:
    name: ollama-webui_ollama-webui
