name: ollama-webui
services:
  ollama:
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities:
                - gpu
              driver: nvidia
              count: 1
    image: ollama/ollama:latest
    networks:
      default: null
    ports:
      - mode: ingress
        target: 11434
        published: "11434"
        protocol: tcp
    pull_policy: always
    restart: unless-stopped
    tty: true
    volumes:
      - type: bind
        source: /media/lpetrov/4TB/opt/ollama-data
        target: /root/.ollama
        bind:
          create_host_path: true
  open-webui:
    build:
      context: /home/lpetrov/projects/sandbox/ai/ollama-webui
      dockerfile: Dockerfile
      args:
        OLLAMA_API_BASE_URL: /ollama/api
    container_name: open-webui
    depends_on:
      ollama:
        condition: service_started
        required: true
    environment:
      OLLAMA_API_BASE_URL: http://ollama:11434/api
      WEBUI_SECRET_KEY: ""
    extra_hosts:
      - host.docker.internal=host-gateway
    image: ghcr.io/open-webui/open-webui:main
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8080
        published: "3000"
        protocol: tcp
    restart: unless-stopped
    volumes:
      - type: volume
        source: open-webui
        target: /app/backend/data
        volume: {}
networks:
  default:
    name: ollama-webui_default
volumes:
  open-webui:
    name: ollama-webui_open-webui
